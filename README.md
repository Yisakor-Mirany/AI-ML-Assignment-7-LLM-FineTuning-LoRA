#AI-ML-Assignment-7-LLM-FineTuning-LoRA

Author: Yisakor Mirany
Model: distilbert-base-uncased
PEFT Method: LoRA (Low-Rank Adaptation)
Dataset: IMDB (Sentiment Analysis)

ðŸ” Summary

This project applies Parameter-Efficient Fine-Tuning (PEFT) using LoRA to fine-tune a pre-trained LLM (DistilBERT) for a binary sentiment classification task on the IMDB movie reviews dataset.

Instead of updating all 66M+ model parameters, LoRA injects small trainable rank-decomposition matrices into the attention mechanism, drastically reducing compute requirements while still achieving high accuracy.

The system:

Loads and preprocesses the IMDB dataset

Tokenizes text using the DistilBERT tokenizer

Attaches LoRA adapters to the query/value matrices

Fine-tunes only 1â€“2% of the model parameters

Evaluates model performance on Accuracy and F1

Saves final LoRA-enabled sentiment classifier

ðŸ“¦ Project Components

notebook.ipynb â€“ Full workflow (data prep â†’ LoRA setup â†’ training â†’ evaluation)

requirements.txt â€“ Python dependencies

README.md â€“ Documentation

Saved Model Folder (optional) â€“ Final LoRA-tuned DistilBERT model

ðŸ“˜ Model & LoRA Configuration

Base Model: distilbert-base-uncased
Task: Sentiment classification
Classes:

0 â†’ Negative

1 â†’ Positive

LoRA Settings:

Parameter	Value
r	8
lora_alpha	16
lora_dropout	0.1
Target	Query & Value projections
PEFT Task Type	Sequence Classification
ðŸ“Š Training Details

Epochs: 3

Batch Size: 16

Optimizer: AdamW (via HF Trainer)

Precision: FP16 (mixed precision)

Training Strategy: Evaluation + checkpoint each epoch

The notebook includes the complete training loop using Hugging Face Trainer.

ðŸ§ª Evaluation Metrics

After training, the model is evaluated on the IMDB test set.

Metric	Score
Accuracy	(Insert your final score here)
F1 Score	(Insert your final score here)

These scores typically reach ~90â€“93% with LoRA on DistilBERT.

âš–ï¸ Baseline vs Fine-Tuned Comparison
Baseline (zero-shot DistilBERT):

Accuracy: ~50â€“60%

No task-specific knowledge

Poor sentiment classification

LoRA Fine-Tuned DistilBERT:

Accuracy: >90%

F1 significantly improves

Learns sentiment cues effectively

Only trains a tiny subset of parameters

âš¡ Why LoRA Is Efficient

LoRA dramatically reduces the cost of fine-tuning by:

Freezing the entire pre-trained model

Injecting small, low-rank adapters into attention layers

Training only ~1â€“2% of parameters

Reducing GPU memory requirements

Speeding up training without losing accuracy

This makes it ideal for student environments, Colab GPUs, and real-world low-resource scenarios.
